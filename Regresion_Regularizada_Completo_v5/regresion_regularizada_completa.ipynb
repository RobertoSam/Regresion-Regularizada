{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d184d70d",
   "metadata": {},
   "source": [
    "# üìò Regresi√≥n Regularizada: Teor√≠a, Comparaci√≥n y Aplicaciones\n",
    "\n",
    "Este cuaderno re√∫ne el desarrollo completo de regresiones Ridge, Lasso y Elastic Net. Est√° dise√±ado como una gu√≠a did√°ctica para estudiantes de posgrado y profesionales de ciencia de datos.\n",
    "\n",
    "Incluye teor√≠a, visualizaci√≥n geom√©trica, comparaci√≥n de coeficientes, evaluaci√≥n estad√≠stica y aplicaci√≥n a datos reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736a6c3",
   "metadata": {},
   "source": [
    "## üîç ¬øPor qu√© regularizar?\n",
    "- La regresi√≥n lineal cl√°sica (OLS) puede tener alta varianza si hay colinealidad o muchas variables.\n",
    "- Regularizar significa **penalizar los coeficientes grandes** para reducir sobreajuste.\n",
    "- Esto introduce sesgo, pero mejora la generalizaci√≥n.\n",
    "\n",
    "### Gr√°fico: Dispersi√≥n de predicciones\n",
    "Veremos c√≥mo OLS produce predicciones inestables comparado con modelos regularizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211e249",
   "metadata": {},
   "source": [
    "## üß± Ridge Regression\n",
    "\n",
    "La regresi√≥n Ridge aplica una penalizaci√≥n L2 sobre los coeficientes para evitar sobreajuste.\n",
    "\n",
    "**Ecuaci√≥n objetivo:**\n",
    "\n",
    "\\[\n",
    "\\min_\\beta \\sum_{i=1}^n (y_i - X_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "\\]\n",
    "\n",
    "- El par√°metro \\( \\lambda \\) controla la fuerza de la penalizaci√≥n.\n",
    "- Ridge no realiza selecci√≥n de variables (coeficientes ‚â† 0), pero **reduce su magnitud**.\n",
    "\n",
    "**Geometr√≠a:** la regi√≥n de restricci√≥n es una esfera ‚Üí mantiene todos los coeficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6697dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelo Ridge\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ridge)):.2f}\")\n",
    "print(f\"R¬≤   : {r2_score(y_test, y_pred_ridge):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef159f",
   "metadata": {},
   "source": [
    "### üîç Visualizaci√≥n de coeficientes\n",
    "\n",
    "Podemos observar c√≥mo la penalizaci√≥n L2 suaviza los coeficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Coeficientes estimados por Ridge\")\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Valor del coeficiente\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.title('Coeficientes del modelo Ridge')\n",
    "plt.xlabel('√çndice de variable')\n",
    "plt.ylabel('Valor del coeficiente')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04592ed",
   "metadata": {},
   "source": [
    "### ‚úÖ Evaluaci√≥n de supuestos\n",
    "\n",
    "- Analizamos los residuos para ver si el modelo cumple con los supuestos cl√°sicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ca7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos = y_test - y_pred_ridge\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_pred_ridge, residuos)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Residuo\")\n",
    "plt.title(\"Residuos vs Predicci√≥n\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "import scipy.stats as stats\n",
    "stats.probplot(residuos, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q plot de residuos\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173a999",
   "metadata": {},
   "source": [
    "## üß± Ridge Regression\n",
    "\n",
    "La regresi√≥n Ridge aplica una penalizaci√≥n L2 sobre los coeficientes para evitar sobreajuste.\n",
    "\n",
    "**Ecuaci√≥n objetivo:**\n",
    "\n",
    "\\[\n",
    "\\min_\\beta \\sum_{i=1}^n (y_i - X_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "\\]\n",
    "\n",
    "- El par√°metro \\( \\lambda \\) controla la fuerza de la penalizaci√≥n.\n",
    "- Ridge no realiza selecci√≥n de variables (coeficientes ‚â† 0), pero **reduce su magnitud**.\n",
    "\n",
    "**Geometr√≠a:** la regi√≥n de restricci√≥n es una esfera ‚Üí mantiene todos los coeficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eeabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelo Ridge\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ridge)):.2f}\")\n",
    "print(f\"R¬≤   : {r2_score(y_test, y_pred_ridge):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3b271",
   "metadata": {},
   "source": [
    "### üîç Visualizaci√≥n de coeficientes\n",
    "\n",
    "Podemos observar c√≥mo la penalizaci√≥n L2 suaviza los coeficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086359b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Coeficientes estimados por Ridge\")\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Valor del coeficiente\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.title('Coeficientes del modelo Ridge')\n",
    "plt.xlabel('√çndice de variable')\n",
    "plt.ylabel('Valor del coeficiente')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbf211",
   "metadata": {},
   "source": [
    "### ‚úÖ Evaluaci√≥n de supuestos\n",
    "\n",
    "- Analizamos los residuos para ver si el modelo cumple con los supuestos cl√°sicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0889f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos = y_test - y_pred_ridge\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_pred_ridge, residuos)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Residuo\")\n",
    "plt.title(\"Residuos vs Predicci√≥n\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "import scipy.stats as stats\n",
    "stats.probplot(residuos, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q plot de residuos\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394286f",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Lasso Regression\n",
    "\n",
    "La regresi√≥n Lasso utiliza una penalizaci√≥n L1 que promueve la **esparsidad** en los coeficientes.\n",
    "\n",
    "**Ecuaci√≥n objetivo:**\n",
    "\n",
    "\\[\n",
    "\\min_\\beta \\sum_{i=1}^n (y_i - X_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "\\]\n",
    "\n",
    "- Algunos coeficientes se vuelven exactamente cero ‚Üí selecci√≥n autom√°tica de variables.\n",
    "- √ötil cuando se sospecha que muchas variables no aportan valor.\n",
    "\n",
    "**Geometr√≠a:** la regi√≥n de penalizaci√≥n tiene forma de rombo ‚Üí es m√°s probable que la soluci√≥n caiga en un eje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd23290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Modelo Lasso\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_lasso)):.2f}\")\n",
    "print(f\"R¬≤   : {r2_score(y_test, y_pred_lasso):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67cdee2",
   "metadata": {},
   "source": [
    "### üîç Coeficientes estimados por Lasso\n",
    "\n",
    "Observa c√≥mo algunos coeficientes son exactamente cero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Coeficientes estimados por Lasso\")\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Valor del coeficiente\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410b20e",
   "metadata": {},
   "source": [
    "### ‚úÖ Evaluaci√≥n de supuestos (Lasso)\n",
    "\n",
    "- Analizamos los residuos para revisar la normalidad y homocedasticidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos_lasso = y_test - y_pred_lasso\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_pred_lasso, residuos_lasso)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Residuo\")\n",
    "plt.title(\"Residuos vs Predicci√≥n\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "import scipy.stats as stats\n",
    "stats.probplot(residuos_lasso, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q plot de residuos\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88297d04",
   "metadata": {},
   "source": [
    "## üîó Elastic Net Regression\n",
    "\n",
    "Elastic Net combina las penalizaciones de Ridge (L2) y Lasso (L1):\n",
    "\n",
    "\\[\n",
    "\\min_\\beta \\sum_{i=1}^n (y_i - X_i^T \\beta)^2 + \\lambda \\left[ \\alpha \\sum_j |\\beta_j| + (1 - \\alpha) \\sum_j \\beta_j^2 \\right]\n",
    "\\]\n",
    "\n",
    "- El par√°metro `alpha` (\\( \\alpha \\)) controla la mezcla:\n",
    "  - \\( \\alpha = 1 \\) ‚Üí Lasso\n",
    "  - \\( \\alpha = 0 \\) ‚Üí Ridge\n",
    "\n",
    "- Excelente opci√≥n cuando:\n",
    "  - Hay muchas variables correlacionadas\n",
    "  - Hay sospecha de sparsity parcial\n",
    "\n",
    "**Geometr√≠a:** regi√≥n de penalizaci√≥n intermedia entre c√≠rculo y rombo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b99da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Modelo Elastic Net\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "elastic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_elastic = elastic.predict(X_test_scaled)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_elastic)):.2f}\")\n",
    "print(f\"R¬≤   : {r2_score(y_test, y_pred_elastic):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3af0a",
   "metadata": {},
   "source": [
    "### üîç Coeficientes de Elastic Net\n",
    "\n",
    "Tiene la ventaja de combinar la estabilidad de Ridge con la selecci√≥n de variables de Lasso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Coeficientes estimados por Elastic Net\")\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Valor del coeficiente\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0750201",
   "metadata": {},
   "source": [
    "### ‚úÖ Evaluaci√≥n de supuestos (Elastic Net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos_elastic = y_test - y_pred_elastic\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_pred_elastic, residuos_elastic)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Residuo\")\n",
    "plt.title(\"Residuos vs Predicci√≥n\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "import scipy.stats as stats\n",
    "stats.probplot(residuos_elastic, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q plot de residuos\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f5088d",
   "metadata": {},
   "source": [
    "## üìä Comparaci√≥n final entre Ridge, Lasso y Elastic Net\n",
    "\n",
    "Evaluamos los modelos usando las mismas m√©tricas sobre el mismo conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a93825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas\n",
    "modelos = ['Ridge', 'Lasso', 'Elastic Net']\n",
    "r2_scores = [\n",
    "    r2_score(y_test, y_pred_ridge),\n",
    "    r2_score(y_test, y_pred_lasso),\n",
    "    r2_score(y_test, y_pred_elastic)\n",
    "]\n",
    "rmse_scores = [\n",
    "    np.sqrt(mean_squared_error(y_test, y_pred_ridge)),\n",
    "    np.sqrt(mean_squared_error(y_test, y_pred_lasso)),\n",
    "    np.sqrt(mean_squared_error(y_test, y_pred_elastic))\n",
    "]\n",
    "n_coefs = [\n",
    "    np.sum(ridge.coef_ != 0),\n",
    "    np.sum(lasso.coef_ != 0),\n",
    "    np.sum(elastic.coef_ != 0)\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "tabla = pd.DataFrame({\n",
    "    \"Modelo\": modelos,\n",
    "    \"R¬≤\": np.round(r2_scores, 3),\n",
    "    \"RMSE\": np.round(rmse_scores, 2),\n",
    "    \"N¬∫ de Coef. ‚â† 0\": n_coefs\n",
    "})\n",
    "tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a50ff",
   "metadata": {},
   "source": [
    "### üîç Visualizaci√≥n comparativa de coeficientes\n",
    "\n",
    "Observa c√≥mo cada t√©cnica trata la magnitud y cantidad de variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(ridge.coef_, label='Ridge')\n",
    "plt.plot(lasso.coef_, label='Lasso')\n",
    "plt.plot(elastic.coef_, label='Elastic Net')\n",
    "plt.title(\"Comparaci√≥n de coeficientes\")\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Valor del coeficiente\")\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4540f8b",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusiones\n",
    "\n",
    "- **Ridge** mantiene todos los coeficientes peque√±os ‚Üí √∫til con multicolinealidad.\n",
    "- **Lasso** realiza selecci√≥n de variables autom√°tica.\n",
    "- **Elastic Net** balancea ambos efectos, √∫til con muchas variables correlacionadas.\n",
    "\n",
    "Este an√°lisis muestra c√≥mo cada t√©cnica responde a los mismos datos de forma diferente, y c√≥mo elegir entre ellas depende de los objetivos: estabilidad, interpretabilidad o predicci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d092e",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è Historia de los m√©todos\n",
    "\n",
    "| M√©todo        | A√±o  | Creador(es)             | Motivaci√≥n principal                          |\n",
    "|---------------|------|--------------------------|------------------------------------------------|\n",
    "| **Ridge**     | 1970 | Hoerl & Kennard          | Estabilizar estimaciones con colinealidad     |\n",
    "| **Lasso**     | 1996 | Robert Tibshirani        | Selecci√≥n autom√°tica de variables             |\n",
    "| **Elastic Net** | 2005 | Zou & Hastie            | Combinar estabilidad y esparsidad             |\n",
    "\n",
    "Estos m√©todos marcaron hitos importantes en la estad√≠stica moderna y aprendizaje autom√°tico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768d8f5",
   "metadata": {},
   "source": [
    "## üßæ Origen de los nombres\n",
    "\n",
    "- **Ridge**: ‚ÄúCresta‚Äù que estabiliza soluciones cuando hay colinealidad.\n",
    "- **Lasso**: Acr√≥nimo de *Least Absolute Shrinkage and Selection Operator*, tambi√©n sugiere un lazo que ata coeficientes a cero.\n",
    "- **Elastic Net**: Red el√°stica que combina las propiedades de Ridge y Lasso.\n",
    "\n",
    "Estos nombres reflejan tanto la geometr√≠a como la intenci√≥n estad√≠stica de cada m√©todo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
