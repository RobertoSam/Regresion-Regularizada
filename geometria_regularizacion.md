# üìê Geometr√≠a de la Regularizaci√≥n

La regularizaci√≥n en regresi√≥n lineal introduce restricciones sobre los coeficientes del modelo mediante **penalizaciones**, que pueden interpretarse como **formas geom√©tricas** en el espacio de par√°metros. Estas formas afectan directamente c√≥mo se obtiene la soluci√≥n √≥ptima.

---

## üîç Intuici√≥n geom√©trica

Imaginemos que minimizamos el error cuadr√°tico bajo una restricci√≥n en los coeficientes Œ≤. Cada tipo de regularizaci√≥n define una "regi√≥n" alrededor del origen dentro de la cual buscamos la mejor soluci√≥n.

### üü¶ Ridge: Penalizaci√≥n L2

- **Regi√≥n geom√©trica**: **C√≠rculo** o **esfera** en dimensiones mayores.
- **Penalizaci√≥n**: `||Œ≤||¬≤‚ÇÇ ‚â§ t`
- **Efecto**: Empuja los coeficientes hacia cero, pero raramente exactamente cero.
- **Visualizaci√≥n**: Las curvas de error (√≥valos) intersectan el c√≠rculo suavemente ‚Üí todos los coeficientes son peque√±os.

### ‚óºÔ∏è Lasso: Penalizaci√≥n L1

- **Regi√≥n geom√©trica**: **Rombo** o **octaedro** en alta dimensi√≥n.
- **Penalizaci√≥n**: `||Œ≤||‚ÇÅ ‚â§ t`
- **Efecto**: Favorece soluciones esparsas ‚Üí muchos coeficientes exactamente cero.
- **Visualizaci√≥n**: Los v√©rtices del rombo aumentan la probabilidad de "chocar" en el borde ‚Üí selecci√≥n de variables.

### üî∑ Elastic Net: Combinaci√≥n L1 + L2

- **Regi√≥n geom√©trica**: Forma **curva intermedia** entre c√≠rculo y rombo.
- **Penalizaci√≥n**: `Œ±||Œ≤||‚ÇÅ + (1 - Œ±)||Œ≤||¬≤‚ÇÇ`
- **Efecto**: Control balanceado entre reducci√≥n de coeficientes y esparsidad.
- **Visualizaci√≥n**: Superficie de intersecci√≥n que combina los efectos anteriores.

---

## üñºÔ∏è Representaci√≥n visual

![Geometr√≠a de Ridge, Lasso y Elastic Net](graficos_regularizacion/A_2D_digital_illustration_features_three_geometric.png)

La imagen compara visualmente las regiones de penalizaci√≥n para cada t√©cnica. El punto √≥ptimo del modelo (m√≠nimo del error cuadr√°tico) se proyecta dentro de estas regiones:

- En **Ridge**, la soluci√≥n suaviza todos los coeficientes.
- En **Lasso**, algunos coeficientes se hacen cero.
- En **Elastic Net**, hay un compromiso entre ambos extremos.

---

## üìö Lecturas recomendadas

- T. Hastie, R. Tibshirani, J. Friedman ‚Äî *The Elements of Statistical Learning*.
- G. James et al. ‚Äî *An Introduction to Statistical Learning*.
- Zou & Hastie (2005) ‚Äî *Regularization and variable selection via the elastic net*.

---

Esta geometr√≠a explica **por qu√©** Lasso selecciona variables, Ridge estabiliza soluciones, y Elastic Net combina lo mejor de ambos.
