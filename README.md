# ğŸ“Š Regresiones Regularizadas: Ridge, Lasso y Elastic Net

Este repositorio contiene una unidad completa sobre regresiÃ³n regularizada en modelos lineales, orientada a estudiantes de maestrÃ­a y profesionales en ciencia de datos y modelaciÃ³n matemÃ¡tica.

---

## ğŸ“ Contenido del repositorio

| Archivo | DescripciÃ³n |
|--------|-------------|
| `regresion_regularizada_completa.ipynb` | Notebook integral con teorÃ­a, aplicaciÃ³n y comparaciÃ³n de Ridge, Lasso y Elastic Net |
| `ridge_regression.ipynb` | TeorÃ­a, visualizaciÃ³n e implementaciÃ³n completa de **RegresiÃ³n Ridge** |
| `lasso_regression.ipynb` | TeorÃ­a e implementaciÃ³n de **RegresiÃ³n Lasso**, con enfoque en selecciÃ³n de variables |
| `elasticnet_regression.ipynb` | AnÃ¡lisis de **Elastic Net**, mostrando cÃ³mo combina Ridge y Lasso |
| `comparacion_modelos_regularizados.ipynb` | ComparaciÃ³n prÃ¡ctica y grÃ¡fica entre los tres modelos |
| `graficos_regularizacion/` | ImÃ¡genes explicativas sobre las regiones geomÃ©tricas de cada modelo |
| `geometria_regularizacion.md` | ExplicaciÃ³n detallada sobre la geometrÃ­a de la regularizaciÃ³n |

---

## ğŸ¯ Objetivos de aprendizaje

- Comprender las limitaciones de la regresiÃ³n OLS y cÃ³mo la regularizaciÃ³n las mitiga.
- Conocer el fundamento teÃ³rico y geomÃ©trico de Ridge, Lasso y Elastic Net.
- Aplicar cada tÃ©cnica en un ejemplo real, analizar sus coeficientes y evaluar su rendimiento.
- Validar los supuestos del modelo usando anÃ¡lisis de residuos.
- Comparar los tres mÃ©todos y justificar su elecciÃ³n en problemas reales.

---

## ğŸ§  GeometrÃ­a de la regularizaciÃ³n

Cuando aplicamos regularizaciÃ³n en regresiÃ³n lineal, aÃ±adimos una restricciÃ³n que **limita el tamaÃ±o de los coeficientes**. Esta restricciÃ³n define una **regiÃ³n geomÃ©trica** en el espacio de parÃ¡metros, y varÃ­a segÃºn el tipo de penalizaciÃ³n:

| MÃ©todo        | PenalizaciÃ³n                                | RegiÃ³n geomÃ©trica        | Efecto tÃ­pico                                      |
|---------------|---------------------------------------------|---------------------------|----------------------------------------------------|
| **Ridge**     | âˆ‘ Î²Â²  (norma L2)                            | CÃ­rculo / Esfera          | Coeficientes pequeÃ±os, raramente exactamente cero |
| **Lasso**     | âˆ‘ |Î²| (norma L1)                            | Rombos / Poliedros        | Algunos coeficientes exactamente cero             |
| **Elastic Net**| Î± âˆ‘ |Î²| + (1 - Î±) âˆ‘ Î²Â² (mixta L1 + L2)     | RegiÃ³n curva intermedia   | SelecciÃ³n + estabilidad con variables correlacionadas |

ğŸ“Œ La soluciÃ³n del modelo se encuentra en el punto donde la elipse de error (curva de nivel de la pÃ©rdida cuadrÃ¡tica) **toca** el borde de esta regiÃ³n.

![GeometrÃ­a Ridge, Lasso y Elastic Net](graficos_regularizacion/A_2D_digital_illustration_features_three_geometric.png)

> Las formas geomÃ©tricas ayudan a explicar:
> - Por quÃ© **Lasso** elimina coeficientes (por sus vÃ©rtices angulosos).
> - Por quÃ© **Ridge** los reduce sin hacerlos cero (por su borde suave).
> - CÃ³mo **Elastic Net** combina ambos comportamientos segÃºn Î±.

ğŸ“„ Para una explicaciÃ³n teÃ³rica completa con fÃ³rmulas, intuiciÃ³n y referencias:  
[geometria_regularizacion.md](geometria_regularizacion.md)

---

## âš™ï¸ TecnologÃ­as utilizadas

- Python 3.11
- Scikit-learn
- Numpy, Matplotlib, Seaborn
- Statsmodels, Scipy

---

## ğŸš€ Ejecutar en lÃ­nea

- [Notebook completo en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/regresion_regularizada_completa_final.ipynb)
- [Abrir Ridge en Google Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/ridge_regression.ipynb)
- [Abrir Lasso en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/lasso_regression.ipynb)
- [Abrir Elastic Net en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/elasticnet_regression.ipynb)
- [ComparaciÃ³n en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/comparacion_modelos_regularizados.ipynb)

---

## ğŸ›  InstalaciÃ³n local rÃ¡pida

1. Clona este repositorio:
```bash
git clone https://github.com/RobertoSam/Regresion-Regularizada.git
cd Regresion-Regularizada
```

2. Crea un entorno virtual (opcional pero recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. Instala las dependencias:
```bash
pip install -r requirements.txt
```

4. Abre los notebooks:
```bash
jupyter notebook
```

---

## ğŸ“š Aplicaciones actuales

Las tÃ©cnicas de regresiÃ³n regularizada se usan ampliamente en:

- GenÃ³mica y biologÃ­a computacional
- Marketing predictivo
- SelecciÃ³n de variables en modelos con alta dimensionalidad
- Modelos de series temporales con mÃºltiples covariables

---

## ğŸ‘¨â€ğŸ« Autor

Material desarrollado por **Roberto Sam** para la MaestrÃ­a en ModelizaciÃ³n MatemÃ¡tica y Computacional â€“ Universidad Nacional de IngenierÃ­a (UNI), PerÃº.  
Asistencia tÃ©cnica y didÃ¡ctica desarrollada con ayuda de un asistente acadÃ©mico especializado en matemÃ¡ticas aplicadas y ciencia de datos.
