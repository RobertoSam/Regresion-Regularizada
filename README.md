# ğŸ“Š Regresiones Regularizadas: Ridge, Lasso y Elastic Net

Este repositorio contiene una unidad completa sobre regresiÃ³n regularizada en modelos lineales, orientada a estudiantes de maestrÃ­a y profesionales en ciencia de datos y modelaciÃ³n matemÃ¡tica.

---

## ğŸ“ Contenido del repositorio

| Archivo | DescripciÃ³n |
|--------|-------------|
| `regresion_regularizada_completa.ipynb` | Notebook integral con teorÃ­a, aplicaciÃ³n y comparaciÃ³n de Ridge, Lasso y Elastic Net |
| `ridge_regression.ipynb` | TeorÃ­a, visualizaciÃ³n e implementaciÃ³n completa de **RegresiÃ³n Ridge** |
| `lasso_regression.ipynb` | TeorÃ­a e implementaciÃ³n de **RegresiÃ³n Lasso**, con enfoque en selecciÃ³n de variables |
| `elasticnet_regression.ipynb` | AnÃ¡lisis de **Elastic Net**, mostrando cÃ³mo combina Ridge y Lasso |
| `comparacion_modelos_regularizados.ipynb` | ComparaciÃ³n prÃ¡ctica y grÃ¡fica entre los tres modelos |
| `graficos_regularizacion/` | ImÃ¡genes explicativas sobre las regiones geomÃ©tricas de cada modelo |
| `geometria_regularizacion.md` | ExplicaciÃ³n detallada sobre la geometrÃ­a de la regularizaciÃ³n |

---

## ğŸ¯ Objetivos de aprendizaje

- Comprender las limitaciones de la regresiÃ³n OLS y cÃ³mo la regularizaciÃ³n las mitiga.
- Conocer el fundamento teÃ³rico y geomÃ©trico de Ridge, Lasso y Elastic Net.
- Aplicar cada tÃ©cnica en un ejemplo real, analizar sus coeficientes y evaluar su rendimiento.
- Validar los supuestos del modelo usando anÃ¡lisis de residuos.
- Comparar los tres mÃ©todos y justificar su elecciÃ³n en problemas reales.

---

## ğŸ§  GeometrÃ­a de la regularizaciÃ³n

Cuando aplicamos regularizaciÃ³n en regresiÃ³n lineal, aÃ±adimos una restricciÃ³n que **limita el tamaÃ±o de los coeficientes**. Esta restricciÃ³n impone una regiÃ³n geomÃ©trica que depende de la norma utilizada:

| MÃ©todo         | PenalizaciÃ³n               | RegiÃ³n geomÃ©trica      | Efecto tÃ­pico                               |
|----------------|----------------------------|-------------------------|---------------------------------------------|
| **Ridge**      | \( \sum \beta_j^2 \)       | CÃ­rculo / Esfera        | Coeficientes pequeÃ±os, ninguno exactamente cero |
| **Lasso**      | \( \sum |\beta_j| \)       | Rombos / Poliedros      | Algunos coeficientes exactamente cero       |
| **Elastic Net**| \( \alpha L1 + (1 - \alpha) L2 \) | RegiÃ³n intermedia | CombinaciÃ³n de selecciÃ³n y estabilidad      |

ğŸ“Œ El punto Ã³ptimo se encuentra donde la elipse de error (funciÃ³n de pÃ©rdida) toca esta regiÃ³n de penalizaciÃ³n:

![GeometrÃ­a Ridge, Lasso y Elastic Net](graficos_regularizacion/A_2D_digital_illustration_features_three_geometric.png)

> Las formas geomÃ©tricas ayudan a entender por quÃ© **Lasso tiende a eliminar variables** (vÃ©rtices angulados del rombo) y **Ridge las conserva pequeÃ±as** (borde suave de la esfera).

ğŸ” Para una explicaciÃ³n mÃ¡s profunda, incluyendo fÃ³rmulas, teorÃ­a y bibliografÃ­a, consulta el archivo:

ğŸ“„ [`geometria_regularizacion.md`](geometria_regularizacion.md)

---

## âš™ï¸ TecnologÃ­as utilizadas

- Python 3.11
- Scikit-learn
- Numpy, Matplotlib, Seaborn
- Statsmodels, Scipy

---

## ğŸš€ Ejecutar en lÃ­nea

- [Notebook completo en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/regresion_regularizada_completa_final.ipynb)
- [Abrir Ridge en Google Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/ridge_regression.ipynb)
- [Abrir Lasso en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/lasso_regression.ipynb)
- [Abrir Elastic Net en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/elasticnet_regression.ipynb)
- [ComparaciÃ³n en Colab](https://colab.research.google.com/github/RobertoSam/Regresion-Regularizada/blob/main/comparacion_modelos_regularizados.ipynb)

---

## ğŸ›  InstalaciÃ³n local rÃ¡pida

1. Clona este repositorio:
```bash
git clone https://github.com/RobertoSam/Regresion-Regularizada.git
cd Regresion-Regularizada
```

2. Crea un entorno virtual (opcional pero recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. Instala las dependencias:
```bash
pip install -r requirements.txt
```

4. Abre los notebooks:
```bash
jupyter notebook
```

---

## ğŸ“š Aplicaciones actuales

Las tÃ©cnicas de regresiÃ³n regularizada se usan ampliamente en:

- GenÃ³mica y biologÃ­a computacional
- Marketing predictivo
- SelecciÃ³n de variables en modelos con alta dimensionalidad
- Modelos de series temporales con mÃºltiples covariables

---

## ğŸ‘¨â€ğŸ« Autor

Material desarrollado por **Roberto Sam** para la MaestrÃ­a en ModelizaciÃ³n MatemÃ¡tica y Computacional â€“ Universidad Nacional de IngenierÃ­a (UNI), PerÃº.  
Asistencia tÃ©cnica y didÃ¡ctica desarrollada con ayuda de un asistente acadÃ©mico especializado en matemÃ¡ticas aplicadas y ciencia de datos.
