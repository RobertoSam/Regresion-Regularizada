{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Regresi\u00f3n Lasso: Teor\u00eda y Aplicaci\u00f3n\n", "\n", "Este notebook explora la regresi\u00f3n Lasso, con \u00e9nfasis en su capacidad de seleccionar variables, teor\u00eda subyacente, implementaci\u00f3n pr\u00e1ctica y an\u00e1lisis del modelo resultante."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. \u00bfPor qu\u00e9 usar Lasso?\n", "\n", "- Ridge regulariza pero **no elimina** variables.\n", "- En modelos con muchas variables, puede ser \u00fatil seleccionar un subconjunto relevante.\n", "- Lasso usa penalizaci\u00f3n L1, lo que favorece soluciones **esparsas** (muchos coeficientes en cero).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Fundamento te\u00f3rico\n", "La regresi\u00f3n Lasso minimiza:\n", "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda \\|\\beta\\|_1 \\right\\}$$\n", "\n", "- $\\|\\beta\\|_1 = \\sum_j |\\beta_j|$ es la norma L1.\n", "- $\\lambda$ controla la penalizaci\u00f3n.\n", "- Lasso puede poner algunos $\\beta_j = 0$ si no aportan al modelo.\n", "\n", "**Interpretaci\u00f3n geom\u00e9trica:** regi\u00f3n de penalizaci\u00f3n en forma de diamante \u2192 intersecciones en ejes \u2192 coeficientes nulos."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.datasets import load_diabetes\n", "from sklearn.linear_model import Lasso, LinearRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import mean_squared_error, r2_score\n", "import statsmodels.api as sm\n", "from statsmodels.stats.diagnostic import het_breuschpagan\n", "from scipy.stats import shapiro"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Preparar datos\n", "X, y = load_diabetes(return_X_y=True)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "scaler = StandardScaler()\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ajustar Lasso y OLS\n", "lasso = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n", "ols = LinearRegression().fit(X_train_scaled, y_train)\n", "\n", "# Predicci\u00f3n\n", "y_pred_lasso = lasso.predict(X_test_scaled)\n", "y_pred_ols = ols.predict(X_test_scaled)\n", "\n", "# Errores\n", "print(\"Lasso:\", mean_squared_error(y_test, y_pred_lasso), r2_score(y_test, y_pred_lasso))\n", "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Coeficientes\n", "print(\"Coeficientes Lasso:\", lasso.coef_)\n", "print(\"Coeficientes OLS:\", ols.coef_)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Gr\u00e1fico de coeficientes\n", "plt.figure(figsize=(10, 5))\n", "plt.plot(lasso.coef_, label='Lasso')\n", "plt.plot(ols.coef_, label='OLS')\n", "plt.axhline(0, color='gray', linestyle='--')\n", "plt.legend()\n", "plt.title(\"Coeficientes estimados por Lasso vs OLS\")\n", "plt.xlabel(\"\u00cdndice de variable\")\n", "plt.ylabel(\"Valor del coeficiente\")\n", "plt.grid(True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. An\u00e1lisis de residuos y supuestos\n", "Analizamos normalidad, homocedasticidad y comportamiento de errores."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Residuos de Lasso\n", "residuals = y_test - y_pred_lasso\n", "\n", "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n", "sns.histplot(residuals, kde=True, ax=ax[0])\n", "sm.qqplot(residuals, line='s', ax=ax[1])\n", "ax[0].set_title(\"Histograma de residuos\")\n", "ax[1].set_title(\"Q-Q plot\")\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Test de normalidad\n", "stat, p = shapiro(residuals)\n", "print(\"Shapiro-Wilk p-valor:\", p)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Trayectorias de coeficientes con \\( \\lambda \\)\n", "Exploramos c\u00f3mo los coeficientes se vuelven cero conforme aumenta la regularizaci\u00f3n."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alphas = np.logspace(-4, 0.5, 100)\n", "coefs = []\n", "for a in alphas:\n", "    lasso = Lasso(alpha=a, max_iter=10000).fit(X_train_scaled, y_train)\n", "    coefs.append(lasso.coef_)\n", "\n", "plt.figure(figsize=(10, 6))\n", "plt.plot(alphas, coefs)\n", "plt.xscale(\"log\")\n", "plt.xlabel(\"lambda\")\n", "plt.ylabel(\"Coeficientes\")\n", "plt.title(\"Trayectoria de coeficientes en Lasso\")\n", "plt.grid(True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Conclusi\u00f3n y aplicaciones\n", "- Lasso es \u00fatil para selecci\u00f3n de variables y simplificaci\u00f3n del modelo\n", "- Es popular en modelos donde $p \\gg n$\n", "- Se utiliza en gen\u00e9tica, procesamiento de se\u00f1ales, ciencia de datos y econometr\u00eda\n", "- Puede combinarse con Elastic Net para mejorar estabilidad cuando hay colinealidad"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}