{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8524f0",
   "metadata": {},
   "source": [
    "# 🔷 Sección: 01 Intro Analisis Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5be86",
   "metadata": {},
   "source": [
    "# Introducción al problema de regresión con datos de diabetes\n",
    "Este notebook explora un problema clásico de regresión utilizando el conjunto de datos de diabetes incluido en `scikit-learn`. Este análisis es previo al estudio de técnicas de regularización como Ridge, Lasso y Elastic Net, y tiene como propósito entender las características del conjunto de datos, su estructura, y las motivaciones para aplicar técnicas de penalización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3992d",
   "metadata": {},
   "source": [
    "## ¿Qué es el conjunto de datos de diabetes?\n",
    "- El conjunto incluye 442 muestras de pacientes.\n",
    "- Cada muestra tiene 10 características clínicas: edad, sexo, índice de masa corporal (IMC), presión arterial, etc.\n",
    "- Las variables predictoras han sido estandarizadas.\n",
    "- El objetivo (`y`) es una medida cuantitativa de la progresión de la diabetes un año después del diagnóstico.\n",
    "\n",
    "Este es un problema de **regresión multivariada**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857dc2af",
   "metadata": {},
   "source": [
    "## ¿Por qué no basta con regresión lineal ordinaria?\n",
    "- Cuando hay **colinealidad** entre las variables predictoras, la matriz \\( X^T X \\) puede volverse casi singular.\n",
    "- Esto genera **coeficientes inestables** y alta **varianza en las predicciones**.\n",
    "- En contextos con muchas variables o pocos datos, el modelo clásico **sobreajusta** fácilmente.\n",
    "- Las técnicas de regularización ayudan a controlar esto penalizando la complejidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9849ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Cargar dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target, name='disease_progression')\n",
    "df = X.copy()\n",
    "df['target'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a720a",
   "metadata": {},
   "source": [
    "## Distribución de la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b06bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(y, kde=True, bins=30, color='teal')\n",
    "plt.title('Distribución de la progresión de la diabetes')\n",
    "plt.xlabel('Progresión (target)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2d196",
   "metadata": {},
   "source": [
    "## Correlación entre variables predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d989d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt='.2f', square=True)\n",
    "plt.title('Matriz de correlación de variables predictoras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e5cf1",
   "metadata": {},
   "source": [
    "## Ejemplo de colinealidad entre variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.scatterplot(x=X['bmi'], y=X['s5'])\n",
    "plt.title('Relación entre índice de masa corporal (BMI) y S5')\n",
    "plt.xlabel('bmi')\n",
    "plt.ylabel('s5')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca342d",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "Este análisis exploratorio muestra la necesidad de técnicas que mitiguen problemas de varianza alta, multicolinealidad y sobreajuste.\n",
    "A continuación, exploraremos modelos de regresión regularizada como Ridge, Lasso y Elastic Net para abordar estos desafíos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0907557",
   "metadata": {},
   "source": [
    "# 🔷 Sección: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e5d21",
   "metadata": {},
   "source": [
    "# Regresión Ridge: Teoría y Aplicación\n",
    "\n",
    "Este notebook aborda con profundidad la regresión Ridge, incluyendo su motivación, formulación matemática, comparación con OLS, análisis estadístico del modelo y aplicaciones modernas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c3d85",
   "metadata": {},
   "source": [
    "## 1. ¿Por qué regularizar?\n",
    "\n",
    "- En regresión clásica (OLS), si los predictores están altamente correlacionados (colinealidad), la matriz $X^TX$ se vuelve casi singular.\n",
    "- Esto provoca que pequeños cambios en los datos generen grandes variaciones en los coeficientes.\n",
    "- Ridge penaliza los coeficientes grandes para reducir la **varianza** del estimador.\n",
    "\n",
    "### Problemas comunes en OLS:\n",
    "- Alta varianza en coeficientes\n",
    "- Sobreajuste (overfitting)\n",
    "- Inestabilidad numérica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9f3cb",
   "metadata": {},
   "source": [
    "## 2. Fundamento teórico\n",
    "La regresión Ridge minimiza:\n",
    "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda \\|\\beta\\|_2^2 \\right\\}$$\n",
    "\n",
    "Donde:\n",
    "- $\\|\\beta\\|_2^2 = \\sum_j \\beta_j^2$ es la norma L2\n",
    "- $\\lambda$ controla la intensidad de la penalización\n",
    "\n",
    "### Solución analítica:\n",
    "$$\\hat{\\beta}_{\\text{ridge}} = (X^TX + \\lambda I)^{-1} X^Ty$$\n",
    "\n",
    "Cuando $\\lambda=0$ se recupera OLS; cuando $\\lambda\\to\\infty$ los coeficientes se acercan a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y escalar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426150a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Ridge y OLS\n",
    "ridge = Ridge(alpha=10.0).fit(X_train_scaled, y_train)\n",
    "ols = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "y_pred_ols = ols.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "print(\"Ridge:\", mean_squared_error(y_test, y_pred_ridge), r2_score(y_test, y_pred_ridge))\n",
    "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4872286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de residuos de Ridge\n",
    "residuals = y_test - y_pred_ridge\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "sm.qqplot(residuals, line='s', ax=ax[1])\n",
    "ax[0].set_title(\"Histograma de residuos\")\n",
    "ax[1].set_title(\"Q-Q plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test de normalidad\n",
    "stat, p = shapiro(residuals)\n",
    "print(\"Shapiro-Wilk p-valor:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6199a8b",
   "metadata": {},
   "source": [
    "## 3. Exploración de coeficientes vs \\( \\lambda \\)\n",
    "Vamos a observar cómo cambian los coeficientes al aumentar la regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3af52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 4, 100)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a).fit(X_train_scaled, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, coefs)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"Coeficientes\")\n",
    "plt.title(\"Trayectoria de coeficientes en Ridge\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f37c19",
   "metadata": {},
   "source": [
    "## 4. Conclusión y aplicaciones\n",
    "- Ridge estabiliza los coeficientes y reduce sobreajuste\n",
    "- Ideal cuando hay colinealidad entre predictores\n",
    "- Se usa hoy en día en genética, finanzas, modelos predictivos complejos\n",
    "- Base para técnicas más avanzadas como Kernel Ridge y regularización en redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe8ea0",
   "metadata": {},
   "source": [
    "# 🔷 Sección: Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f7f47",
   "metadata": {},
   "source": [
    "# Regresión Lasso: Teoría y Aplicación\n",
    "\n",
    "Este notebook explora la regresión Lasso, con énfasis en su capacidad de seleccionar variables, teoría subyacente, implementación práctica y análisis del modelo resultante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcea776",
   "metadata": {},
   "source": [
    "## 1. ¿Por qué usar Lasso?\n",
    "\n",
    "- Ridge regulariza pero **no elimina** variables.\n",
    "- En modelos con muchas variables, puede ser útil seleccionar un subconjunto relevante.\n",
    "- Lasso usa penalización L1, lo que favorece soluciones **esparsas** (muchos coeficientes en cero).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eba878",
   "metadata": {},
   "source": [
    "## 2. Fundamento teórico\n",
    "La regresión Lasso minimiza:\n",
    "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda \\|\\beta\\|_1 \\right\\}$$\n",
    "\n",
    "- $\\|\\beta\\|_1 = \\sum_j |\\beta_j|$ es la norma L1.\n",
    "- $\\lambda$ controla la penalización.\n",
    "- Lasso puede poner algunos $\\beta_j = 0$ si no aportan al modelo.\n",
    "\n",
    "**Interpretación geométrica:** región de penalización en forma de diamante → intersecciones en ejes → coeficientes nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc72fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413980b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Lasso y OLS\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "ols = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "y_pred_ols = ols.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "print(\"Lasso:\", mean_squared_error(y_test, y_pred_lasso), r2_score(y_test, y_pred_lasso))\n",
    "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0908d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes\n",
    "print(\"Coeficientes Lasso:\", lasso.coef_)\n",
    "print(\"Coeficientes OLS:\", ols.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7721328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de coeficientes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lasso.coef_, label='Lasso')\n",
    "plt.plot(ols.coef_, label='OLS')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title(\"Coeficientes estimados por Lasso vs OLS\")\n",
    "plt.xlabel(\"Índice de variable\")\n",
    "plt.ylabel(\"Valor del coeficiente\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fca1d",
   "metadata": {},
   "source": [
    "## 3. Análisis de residuos y supuestos\n",
    "Analizamos normalidad, homocedasticidad y comportamiento de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuos de Lasso\n",
    "residuals = y_test - y_pred_lasso\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "sm.qqplot(residuals, line='s', ax=ax[1])\n",
    "ax[0].set_title(\"Histograma de residuos\")\n",
    "ax[1].set_title(\"Q-Q plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test de normalidad\n",
    "stat, p = shapiro(residuals)\n",
    "print(\"Shapiro-Wilk p-valor:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c2cca",
   "metadata": {},
   "source": [
    "## 4. Trayectorias de coeficientes con \\( \\lambda \\)\n",
    "Exploramos cómo los coeficientes se vuelven cero conforme aumenta la regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ad47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 0.5, 100)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    lasso = Lasso(alpha=a, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, coefs)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"Coeficientes\")\n",
    "plt.title(\"Trayectoria de coeficientes en Lasso\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1aab0",
   "metadata": {},
   "source": [
    "## 5. Conclusión y aplicaciones\n",
    "- Lasso es útil para selección de variables y simplificación del modelo\n",
    "- Es popular en modelos donde $p \\gg n$\n",
    "- Se utiliza en genética, procesamiento de señales, ciencia de datos y econometría\n",
    "- Puede combinarse con Elastic Net para mejorar estabilidad cuando hay colinealidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d163048",
   "metadata": {},
   "source": [
    "# 🔷 Sección: Elasticnet Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df061c68",
   "metadata": {},
   "source": [
    "# Regresión Elastic Net: Teoría y Aplicación\n",
    "\n",
    "Este notebook explora la regresión Elastic Net, combinando los beneficios de Ridge y Lasso, con teoría, implementación práctica y análisis estadístico del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c178fa",
   "metadata": {},
   "source": [
    "## 1. ¿Por qué usar Elastic Net?\n",
    "\n",
    "- Ridge: buena para colinealidad, pero no hace selección.\n",
    "- Lasso: selecciona variables, pero puede ser inestable si hay variables altamente correlacionadas.\n",
    "- **Elastic Net combina ambas**: selección y agrupamiento de coeficientes correlacionados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1093b",
   "metadata": {},
   "source": [
    "## 2. Fundamento teórico\n",
    "Elastic Net minimiza:\n",
    "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda_1 \\|\\beta\\|_1 + \\lambda_2 \\|\\beta\\|_2^2 \\right\\}$$\n",
    "\n",
    "En `scikit-learn`, se parametriza como:\n",
    "- `alpha`: controla la fuerza total de regularización\n",
    "- `l1_ratio`: pondera entre Lasso (`1.0`) y Ridge (`0.0`)\n",
    "\n",
    "Elastic Net tiende a:\n",
    "- Seleccionar grupos de variables correlacionadas\n",
    "- Mantener estabilidad frente a colinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Elastic Net\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "ols = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_enet = enet.predict(X_test_scaled)\n",
    "y_pred_ols = ols.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "print(\"Elastic Net:\", mean_squared_error(y_test, y_pred_enet), r2_score(y_test, y_pred_enet))\n",
    "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de coeficientes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(enet.coef_, label='Elastic Net')\n",
    "plt.plot(ols.coef_, label='OLS')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title(\"Coeficientes: Elastic Net vs OLS\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6fde1",
   "metadata": {},
   "source": [
    "## 3. Análisis de residuos y supuestos\n",
    "Evaluamos los errores del modelo Elastic Net para validar su consistencia estadística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de residuos\n",
    "residuals = y_test - y_pred_enet\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "sm.qqplot(residuals, line='s', ax=ax[1])\n",
    "ax[0].set_title(\"Histograma de residuos\")\n",
    "ax[1].set_title(\"Q-Q plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk\n",
    "stat, p = shapiro(residuals)\n",
    "print(\"Shapiro-Wilk p-valor:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186de259",
   "metadata": {},
   "source": [
    "## 4. Exploración de l1_ratio\n",
    "Cómo cambian los coeficientes al variar la mezcla entre L1 y L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratios = np.linspace(0.01, 1.0, 20)\n",
    "coefs = []\n",
    "for ratio in l1_ratios:\n",
    "    en = ElasticNet(alpha=0.1, l1_ratio=ratio, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "    coefs.append(en.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(l1_ratios, coefs)\n",
    "plt.xlabel(\"l1_ratio\")\n",
    "plt.ylabel(\"Coeficientes\")\n",
    "plt.title(\"Evolución de coeficientes en Elastic Net\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fe761",
   "metadata": {},
   "source": [
    "## 5. Conclusión y aplicaciones\n",
    "- Elastic Net combina selección de Lasso y estabilidad de Ridge\n",
    "- Se adapta bien cuando hay muchas variables correlacionadas\n",
    "- Muy usado en modelos genómicos, selección de atributos y análisis multivariante\n",
    "- Permite control fino de la estructura del modelo mediante `l1_ratio` y `alpha`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4ef2f",
   "metadata": {},
   "source": [
    "# 🔷 Sección: Comparacion Modelos Regularizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f5644",
   "metadata": {},
   "source": [
    "# Comparación de Regresiones Regularizadas: Ridge, Lasso y Elastic Net\n",
    "\n",
    "Este notebook compara el comportamiento, coeficientes, rendimiento y supuestos de los modelos de regresión regularizada Ridge, Lasso y Elastic Net, usando el mismo conjunto de datos estandarizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b88cef",
   "metadata": {},
   "source": [
    "## 1. Cargar y preparar datos\n",
    "Usaremos el conjunto de datos de diabetes de `sklearn` y lo dividiremos en entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322e7c6",
   "metadata": {},
   "source": [
    "## 2. Ajuste de modelos\n",
    "Utilizaremos valores de hiperparámetros fijos comparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "ridge = Ridge(alpha=1.0).fit(X_train_scaled, y_train)\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "y_pred_enet = enet.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "def resumen(nombre, y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{nombre} → RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "resumen(\"Ridge\", y_test, y_pred_ridge)\n",
    "resumen(\"Lasso\", y_test, y_pred_lasso)\n",
    "resumen(\"Elastic Net\", y_test, y_pred_enet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc91ae",
   "metadata": {},
   "source": [
    "## 3. Comparación de coeficientes\n",
    "Visualizamos las diferencias en magnitud y sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3987fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ridge.coef_, label='Ridge')\n",
    "plt.plot(lasso.coef_, label='Lasso')\n",
    "plt.plot(enet.coef_, label='Elastic Net')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Índice de variable\")\n",
    "plt.ylabel(\"Coeficiente\")\n",
    "plt.title(\"Coeficientes estimados\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d39837",
   "metadata": {},
   "source": [
    "## 4. Número de coeficientes distintos de cero\n",
    "Esto mide cuán esparso es el modelo (selección de variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e3adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coef. distintos de cero:\")\n",
    "print(\"Ridge:\", np.sum(ridge.coef_ != 0))\n",
    "print(\"Lasso:\", np.sum(lasso.coef_ != 0))\n",
    "print(\"Elastic Net:\", np.sum(enet.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040bffd",
   "metadata": {},
   "source": [
    "## ✅ Conclusiones\n",
    "- **Ridge** mantiene todos los coeficientes pequeños → útil con multicolinealidad.\n",
    "- **Lasso** realiza selección de variables automática.\n",
    "- **Elastic Net** balancea ambos efectos, útil con muchas variables correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91592d",
   "metadata": {},
   "source": [
    "## 🕰️ Historia de los métodos\n",
    "| Método        | Año  | Creador(es)             | Motivación principal                          |\n",
    "|---------------|------|--------------------------|------------------------------------------------|\n",
    "| **Ridge**     | 1970 | Hoerl & Kennard          | Estabilizar estimaciones con colinealidad     |\n",
    "| **Lasso**     | 1996 | Robert Tibshirani        | Selección automática de variables             |\n",
    "| **Elastic Net** | 2005 | Zou & Hastie            | Combinar estabilidad y esparsidad             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162c219",
   "metadata": {},
   "source": [
    "## 🧾 Origen de los nombres\n",
    "- **Ridge**: “Cresta” que estabiliza soluciones ante colinealidad.\n",
    "- **Lasso**: Acrónimo de *Least Absolute Shrinkage and Selection Operator*.\n",
    "- **Elastic Net**: Red elástica que combina propiedades de Ridge y Lasso."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
