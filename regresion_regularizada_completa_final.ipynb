{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8524f0",
   "metadata": {},
   "source": [
    "# üî∑ Secci√≥n: 01 Intro Analisis Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5be86",
   "metadata": {},
   "source": [
    "# Introducci√≥n al problema de regresi√≥n con datos de diabetes\n",
    "Este notebook explora un problema cl√°sico de regresi√≥n utilizando el conjunto de datos de diabetes incluido en `scikit-learn`. Este an√°lisis es previo al estudio de t√©cnicas de regularizaci√≥n como Ridge, Lasso y Elastic Net, y tiene como prop√≥sito entender las caracter√≠sticas del conjunto de datos, su estructura, y las motivaciones para aplicar t√©cnicas de penalizaci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3992d",
   "metadata": {},
   "source": [
    "## ¬øQu√© es el conjunto de datos de diabetes?\n",
    "- El conjunto incluye 442 muestras de pacientes.\n",
    "- Cada muestra tiene 10 caracter√≠sticas cl√≠nicas: edad, sexo, √≠ndice de masa corporal (IMC), presi√≥n arterial, etc.\n",
    "- Las variables predictoras han sido estandarizadas.\n",
    "- El objetivo (`y`) es una medida cuantitativa de la progresi√≥n de la diabetes un a√±o despu√©s del diagn√≥stico.\n",
    "\n",
    "Este es un problema de **regresi√≥n multivariada**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857dc2af",
   "metadata": {},
   "source": [
    "## ¬øPor qu√© no basta con regresi√≥n lineal ordinaria?\n",
    "- Cuando hay **colinealidad** entre las variables predictoras, la matriz \\( X^T X \\) puede volverse casi singular.\n",
    "- Esto genera **coeficientes inestables** y alta **varianza en las predicciones**.\n",
    "- En contextos con muchas variables o pocos datos, el modelo cl√°sico **sobreajusta** f√°cilmente.\n",
    "- Las t√©cnicas de regularizaci√≥n ayudan a controlar esto penalizando la complejidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9849ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Cargar dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target, name='disease_progression')\n",
    "df = X.copy()\n",
    "df['target'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a720a",
   "metadata": {},
   "source": [
    "## Distribuci√≥n de la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b06bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(y, kde=True, bins=30, color='teal')\n",
    "plt.title('Distribuci√≥n de la progresi√≥n de la diabetes')\n",
    "plt.xlabel('Progresi√≥n (target)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2d196",
   "metadata": {},
   "source": [
    "## Correlaci√≥n entre variables predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d989d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt='.2f', square=True)\n",
    "plt.title('Matriz de correlaci√≥n de variables predictoras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e5cf1",
   "metadata": {},
   "source": [
    "## Ejemplo de colinealidad entre variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.scatterplot(x=X['bmi'], y=X['s5'])\n",
    "plt.title('Relaci√≥n entre √≠ndice de masa corporal (BMI) y S5')\n",
    "plt.xlabel('bmi')\n",
    "plt.ylabel('s5')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca342d",
   "metadata": {},
   "source": [
    "## Conclusi√≥n\n",
    "Este an√°lisis exploratorio muestra la necesidad de t√©cnicas que mitiguen problemas de varianza alta, multicolinealidad y sobreajuste.\n",
    "A continuaci√≥n, exploraremos modelos de regresi√≥n regularizada como Ridge, Lasso y Elastic Net para abordar estos desaf√≠os."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0907557",
   "metadata": {},
   "source": [
    "# üî∑ Secci√≥n: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e5d21",
   "metadata": {},
   "source": [
    "# Regresi√≥n Ridge: Teor√≠a y Aplicaci√≥n\n",
    "\n",
    "Este notebook aborda con profundidad la regresi√≥n Ridge, incluyendo su motivaci√≥n, formulaci√≥n matem√°tica, comparaci√≥n con OLS, an√°lisis estad√≠stico del modelo y aplicaciones modernas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c3d85",
   "metadata": {},
   "source": [
    "## 1. ¬øPor qu√© regularizar?\n",
    "\n",
    "- En regresi√≥n cl√°sica (OLS), si los predictores est√°n altamente correlacionados (colinealidad), la matriz $X^TX$ se vuelve casi singular.\n",
    "- Esto provoca que peque√±os cambios en los datos generen grandes variaciones en los coeficientes.\n",
    "- Ridge penaliza los coeficientes grandes para reducir la **varianza** del estimador.\n",
    "\n",
    "### Problemas comunes en OLS:\n",
    "- Alta varianza en coeficientes\n",
    "- Sobreajuste (overfitting)\n",
    "- Inestabilidad num√©rica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9f3cb",
   "metadata": {},
   "source": [
    "## 2. Fundamento te√≥rico\n",
    "La regresi√≥n Ridge minimiza:\n",
    "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda \\|\\beta\\|_2^2 \\right\\}$$\n",
    "\n",
    "Donde:\n",
    "- $\\|\\beta\\|_2^2 = \\sum_j \\beta_j^2$ es la norma L2\n",
    "- $\\lambda$ controla la intensidad de la penalizaci√≥n\n",
    "\n",
    "### Soluci√≥n anal√≠tica:\n",
    "$$\\hat{\\beta}_{\\text{ridge}} = (X^TX + \\lambda I)^{-1} X^Ty$$\n",
    "\n",
    "Cuando $\\lambda=0$ se recupera OLS; cuando $\\lambda\\to\\infty$ los coeficientes se acercan a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y escalar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426150a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Ridge y OLS\n",
    "ridge = Ridge(alpha=10.0).fit(X_train_scaled, y_train)\n",
    "ols = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "y_pred_ols = ols.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "print(\"Ridge:\", mean_squared_error(y_test, y_pred_ridge), r2_score(y_test, y_pred_ridge))\n",
    "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4872286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de residuos de Ridge\n",
    "residuals = y_test - y_pred_ridge\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "sm.qqplot(residuals, line='s', ax=ax[1])\n",
    "ax[0].set_title(\"Histograma de residuos\")\n",
    "ax[1].set_title(\"Q-Q plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test de normalidad\n",
    "stat, p = shapiro(residuals)\n",
    "print(\"Shapiro-Wilk p-valor:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6199a8b",
   "metadata": {},
   "source": [
    "## 3. Exploraci√≥n de coeficientes vs \\( \\lambda \\)\n",
    "Vamos a observar c√≥mo cambian los coeficientes al aumentar la regularizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3af52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 4, 100)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a).fit(X_train_scaled, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, coefs)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"Coeficientes\")\n",
    "plt.title(\"Trayectoria de coeficientes en Ridge\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f37c19",
   "metadata": {},
   "source": [
    "## 4. Conclusi√≥n y aplicaciones\n",
    "- Ridge estabiliza los coeficientes y reduce sobreajuste\n",
    "- Ideal cuando hay colinealidad entre predictores\n",
    "- Se usa hoy en d√≠a en gen√©tica, finanzas, modelos predictivos complejos\n",
    "- Base para t√©cnicas m√°s avanzadas como Kernel Ridge y regularizaci√≥n en redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe8ea0",
   "metadata": {},
   "source": [
    "# üî∑ Secci√≥n: Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f7f47",
   "metadata": {},
   "source": [
    "# Regresi√≥n Lasso: Teor√≠a y Aplicaci√≥n\n",
    "\n",
    "Este notebook explora la regresi√≥n Lasso, con √©nfasis en su capacidad de seleccionar variables, teor√≠a subyacente, implementaci√≥n pr√°ctica y an√°lisis del modelo resultante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcea776",
   "metadata": {},
   "source": [
    "## 1. ¬øPor qu√© usar Lasso?\n",
    "\n",
    "- Ridge regulariza pero **no elimina** variables.\n",
    "- En modelos con muchas variables, puede ser √∫til seleccionar un subconjunto relevante.\n",
    "- Lasso usa penalizaci√≥n L1, lo que favorece soluciones **esparsas** (muchos coeficientes en cero).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eba878",
   "metadata": {},
   "source": [
    "## 2. Fundamento te√≥rico\n",
    "La regresi√≥n Lasso minimiza:\n",
    "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda \\|\\beta\\|_1 \\right\\}$$\n",
    "\n",
    "- $\\|\\beta\\|_1 = \\sum_j |\\beta_j|$ es la norma L1.\n",
    "- $\\lambda$ controla la penalizaci√≥n.\n",
    "- Lasso puede poner algunos $\\beta_j = 0$ si no aportan al modelo.\n",
    "\n",
    "**Interpretaci√≥n geom√©trica:** regi√≥n de penalizaci√≥n en forma de diamante ‚Üí intersecciones en ejes ‚Üí coeficientes nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc72fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413980b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Lasso y OLS\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "ols = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "y_pred_ols = ols.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "print(\"Lasso:\", mean_squared_error(y_test, y_pred_lasso), r2_score(y_test, y_pred_lasso))\n",
    "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0908d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes\n",
    "print(\"Coeficientes Lasso:\", lasso.coef_)\n",
    "print(\"Coeficientes OLS:\", ols.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7721328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de coeficientes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lasso.coef_, label='Lasso')\n",
    "plt.plot(ols.coef_, label='OLS')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title(\"Coeficientes estimados por Lasso vs OLS\")\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Valor del coeficiente\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fca1d",
   "metadata": {},
   "source": [
    "## 3. An√°lisis de residuos y supuestos\n",
    "Analizamos normalidad, homocedasticidad y comportamiento de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuos de Lasso\n",
    "residuals = y_test - y_pred_lasso\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "sm.qqplot(residuals, line='s', ax=ax[1])\n",
    "ax[0].set_title(\"Histograma de residuos\")\n",
    "ax[1].set_title(\"Q-Q plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test de normalidad\n",
    "stat, p = shapiro(residuals)\n",
    "print(\"Shapiro-Wilk p-valor:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c2cca",
   "metadata": {},
   "source": [
    "## 4. Trayectorias de coeficientes con \\( \\lambda \\)\n",
    "Exploramos c√≥mo los coeficientes se vuelven cero conforme aumenta la regularizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ad47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 0.5, 100)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    lasso = Lasso(alpha=a, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, coefs)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"Coeficientes\")\n",
    "plt.title(\"Trayectoria de coeficientes en Lasso\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1aab0",
   "metadata": {},
   "source": [
    "## 5. Conclusi√≥n y aplicaciones\n",
    "- Lasso es √∫til para selecci√≥n de variables y simplificaci√≥n del modelo\n",
    "- Es popular en modelos donde $p \\gg n$\n",
    "- Se utiliza en gen√©tica, procesamiento de se√±ales, ciencia de datos y econometr√≠a\n",
    "- Puede combinarse con Elastic Net para mejorar estabilidad cuando hay colinealidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d163048",
   "metadata": {},
   "source": [
    "# üî∑ Secci√≥n: Elasticnet Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df061c68",
   "metadata": {},
   "source": [
    "# Regresi√≥n Elastic Net: Teor√≠a y Aplicaci√≥n\n",
    "\n",
    "Este notebook explora la regresi√≥n Elastic Net, combinando los beneficios de Ridge y Lasso, con teor√≠a, implementaci√≥n pr√°ctica y an√°lisis estad√≠stico del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c178fa",
   "metadata": {},
   "source": [
    "## 1. ¬øPor qu√© usar Elastic Net?\n",
    "\n",
    "- Ridge: buena para colinealidad, pero no hace selecci√≥n.\n",
    "- Lasso: selecciona variables, pero puede ser inestable si hay variables altamente correlacionadas.\n",
    "- **Elastic Net combina ambas**: selecci√≥n y agrupamiento de coeficientes correlacionados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1093b",
   "metadata": {},
   "source": [
    "## 2. Fundamento te√≥rico\n",
    "Elastic Net minimiza:\n",
    "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda_1 \\|\\beta\\|_1 + \\lambda_2 \\|\\beta\\|_2^2 \\right\\}$$\n",
    "\n",
    "En `scikit-learn`, se parametriza como:\n",
    "- `alpha`: controla la fuerza total de regularizaci√≥n\n",
    "- `l1_ratio`: pondera entre Lasso (`1.0`) y Ridge (`0.0`)\n",
    "\n",
    "Elastic Net tiende a:\n",
    "- Seleccionar grupos de variables correlacionadas\n",
    "- Mantener estabilidad frente a colinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Elastic Net\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "ols = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_enet = enet.predict(X_test_scaled)\n",
    "y_pred_ols = ols.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "print(\"Elastic Net:\", mean_squared_error(y_test, y_pred_enet), r2_score(y_test, y_pred_enet))\n",
    "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de coeficientes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(enet.coef_, label='Elastic Net')\n",
    "plt.plot(ols.coef_, label='OLS')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title(\"Coeficientes: Elastic Net vs OLS\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6fde1",
   "metadata": {},
   "source": [
    "## 3. An√°lisis de residuos y supuestos\n",
    "Evaluamos los errores del modelo Elastic Net para validar su consistencia estad√≠stica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de residuos\n",
    "residuals = y_test - y_pred_enet\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "sm.qqplot(residuals, line='s', ax=ax[1])\n",
    "ax[0].set_title(\"Histograma de residuos\")\n",
    "ax[1].set_title(\"Q-Q plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk\n",
    "stat, p = shapiro(residuals)\n",
    "print(\"Shapiro-Wilk p-valor:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186de259",
   "metadata": {},
   "source": [
    "## 4. Exploraci√≥n de l1_ratio\n",
    "C√≥mo cambian los coeficientes al variar la mezcla entre L1 y L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratios = np.linspace(0.01, 1.0, 20)\n",
    "coefs = []\n",
    "for ratio in l1_ratios:\n",
    "    en = ElasticNet(alpha=0.1, l1_ratio=ratio, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "    coefs.append(en.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(l1_ratios, coefs)\n",
    "plt.xlabel(\"l1_ratio\")\n",
    "plt.ylabel(\"Coeficientes\")\n",
    "plt.title(\"Evoluci√≥n de coeficientes en Elastic Net\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fe761",
   "metadata": {},
   "source": [
    "## 5. Conclusi√≥n y aplicaciones\n",
    "- Elastic Net combina selecci√≥n de Lasso y estabilidad de Ridge\n",
    "- Se adapta bien cuando hay muchas variables correlacionadas\n",
    "- Muy usado en modelos gen√≥micos, selecci√≥n de atributos y an√°lisis multivariante\n",
    "- Permite control fino de la estructura del modelo mediante `l1_ratio` y `alpha`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4ef2f",
   "metadata": {},
   "source": [
    "# üî∑ Secci√≥n: Comparacion Modelos Regularizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f5644",
   "metadata": {},
   "source": [
    "# Comparaci√≥n de Regresiones Regularizadas: Ridge, Lasso y Elastic Net\n",
    "\n",
    "Este notebook compara el comportamiento, coeficientes, rendimiento y supuestos de los modelos de regresi√≥n regularizada Ridge, Lasso y Elastic Net, usando el mismo conjunto de datos estandarizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b88cef",
   "metadata": {},
   "source": [
    "## 1. Cargar y preparar datos\n",
    "Usaremos el conjunto de datos de diabetes de `sklearn` y lo dividiremos en entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322e7c6",
   "metadata": {},
   "source": [
    "## 2. Ajuste de modelos\n",
    "Utilizaremos valores de hiperpar√°metros fijos comparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "ridge = Ridge(alpha=1.0).fit(X_train_scaled, y_train)\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "y_pred_enet = enet.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "def resumen(nombre, y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{nombre} ‚Üí RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "resumen(\"Ridge\", y_test, y_pred_ridge)\n",
    "resumen(\"Lasso\", y_test, y_pred_lasso)\n",
    "resumen(\"Elastic Net\", y_test, y_pred_enet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc91ae",
   "metadata": {},
   "source": [
    "## 3. Comparaci√≥n de coeficientes\n",
    "Visualizamos las diferencias en magnitud y sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3987fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ridge.coef_, label='Ridge')\n",
    "plt.plot(lasso.coef_, label='Lasso')\n",
    "plt.plot(enet.coef_, label='Elastic Net')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Coeficiente\")\n",
    "plt.title(\"Coeficientes estimados\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d39837",
   "metadata": {},
   "source": [
    "## 4. N√∫mero de coeficientes distintos de cero\n",
    "Esto mide cu√°n esparso es el modelo (selecci√≥n de variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e3adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coef. distintos de cero:\")\n",
    "print(\"Ridge:\", np.sum(ridge.coef_ != 0))\n",
    "print(\"Lasso:\", np.sum(lasso.coef_ != 0))\n",
    "print(\"Elastic Net:\", np.sum(enet.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040bffd",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusiones\n",
    "- **Ridge** mantiene todos los coeficientes peque√±os ‚Üí √∫til con multicolinealidad.\n",
    "- **Lasso** realiza selecci√≥n de variables autom√°tica.\n",
    "- **Elastic Net** balancea ambos efectos, √∫til con muchas variables correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91592d",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è Historia de los m√©todos\n",
    "| M√©todo        | A√±o  | Creador(es)             | Motivaci√≥n principal                          |\n",
    "|---------------|------|--------------------------|------------------------------------------------|\n",
    "| **Ridge**     | 1970 | Hoerl & Kennard          | Estabilizar estimaciones con colinealidad     |\n",
    "| **Lasso**     | 1996 | Robert Tibshirani        | Selecci√≥n autom√°tica de variables             |\n",
    "| **Elastic Net** | 2005 | Zou & Hastie            | Combinar estabilidad y esparsidad             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162c219",
   "metadata": {},
   "source": [
    "## üßæ Origen de los nombres\n",
    "- **Ridge**: ‚ÄúCresta‚Äù que estabiliza soluciones ante colinealidad.\n",
    "- **Lasso**: Acr√≥nimo de *Least Absolute Shrinkage and Selection Operator*.\n",
    "- **Elastic Net**: Red el√°stica que combina propiedades de Ridge y Lasso."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
