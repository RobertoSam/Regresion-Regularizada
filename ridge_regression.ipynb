{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Regresi\u00f3n Ridge: Teor\u00eda y Aplicaci\u00f3n\n", "\n", "Este notebook aborda con profundidad la regresi\u00f3n Ridge, incluyendo su motivaci\u00f3n, formulaci\u00f3n matem\u00e1tica, comparaci\u00f3n con OLS, an\u00e1lisis estad\u00edstico del modelo y aplicaciones modernas."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. \u00bfPor qu\u00e9 regularizar?\n", "\n", "- En regresi\u00f3n cl\u00e1sica (OLS), si los predictores est\u00e1n altamente correlacionados (colinealidad), la matriz $X^TX$ se vuelve casi singular.\n", "- Esto provoca que peque\u00f1os cambios en los datos generen grandes variaciones en los coeficientes.\n", "- Ridge penaliza los coeficientes grandes para reducir la **varianza** del estimador.\n", "\n", "### Problemas comunes en OLS:\n", "- Alta varianza en coeficientes\n", "- Sobreajuste (overfitting)\n", "- Inestabilidad num\u00e9rica\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Fundamento te\u00f3rico\n", "La regresi\u00f3n Ridge minimiza:\n", "$$\\text{min} \\left\\{ \\| y - X\\beta \\|^2 + \\lambda \\|\\beta\\|_2^2 \\right\\}$$\n", "\n", "Donde:\n", "- $\\|\\beta\\|_2^2 = \\sum_j \\beta_j^2$ es la norma L2\n", "- $\\lambda$ controla la intensidad de la penalizaci\u00f3n\n", "\n", "### Soluci\u00f3n anal\u00edtica:\n", "$$\\hat{\\beta}_{\\text{ridge}} = (X^TX + \\lambda I)^{-1} X^Ty$$\n", "\n", "Cuando $\\lambda=0$ se recupera OLS; cuando $\\lambda\\to\\infty$ los coeficientes se acercan a cero."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.datasets import load_diabetes\n", "from sklearn.linear_model import Ridge, LinearRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import mean_squared_error, r2_score\n", "import statsmodels.api as sm\n", "from statsmodels.stats.diagnostic import het_breuschpagan\n", "from scipy.stats import shapiro"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cargar y escalar datos\n", "X, y = load_diabetes(return_X_y=True)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "scaler = StandardScaler()\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ajustar Ridge y OLS\n", "ridge = Ridge(alpha=10.0).fit(X_train_scaled, y_train)\n", "ols = LinearRegression().fit(X_train_scaled, y_train)\n", "\n", "# Predicci\u00f3n\n", "y_pred_ridge = ridge.predict(X_test_scaled)\n", "y_pred_ols = ols.predict(X_test_scaled)\n", "\n", "# Errores\n", "print(\"Ridge:\", mean_squared_error(y_test, y_pred_ridge), r2_score(y_test, y_pred_ridge))\n", "print(\"OLS:\", mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# An\u00e1lisis de residuos de Ridge\n", "residuals = y_test - y_pred_ridge\n", "\n", "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n", "sns.histplot(residuals, kde=True, ax=ax[0])\n", "sm.qqplot(residuals, line='s', ax=ax[1])\n", "ax[0].set_title(\"Histograma de residuos\")\n", "ax[1].set_title(\"Q-Q plot\")\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Test de normalidad\n", "stat, p = shapiro(residuals)\n", "print(\"Shapiro-Wilk p-valor:\", p)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Exploraci\u00f3n de coeficientes vs \\( \\lambda \\)\n", "Vamos a observar c\u00f3mo cambian los coeficientes al aumentar la regularizaci\u00f3n."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alphas = np.logspace(-4, 4, 100)\n", "coefs = []\n", "for a in alphas:\n", "    ridge = Ridge(alpha=a).fit(X_train_scaled, y_train)\n", "    coefs.append(ridge.coef_)\n", "\n", "plt.figure(figsize=(10, 6))\n", "plt.plot(alphas, coefs)\n", "plt.xscale(\"log\")\n", "plt.xlabel(\"lambda\")\n", "plt.ylabel(\"Coeficientes\")\n", "plt.title(\"Trayectoria de coeficientes en Ridge\")\n", "plt.grid(True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Conclusi\u00f3n y aplicaciones\n", "- Ridge estabiliza los coeficientes y reduce sobreajuste\n", "- Ideal cuando hay colinealidad entre predictores\n", "- Se usa hoy en d\u00eda en gen\u00e9tica, finanzas, modelos predictivos complejos\n", "- Base para t\u00e9cnicas m\u00e1s avanzadas como Kernel Ridge y regularizaci\u00f3n en redes neuronales"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}