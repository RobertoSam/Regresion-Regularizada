{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Comparaci\u00f3n de Regresiones Regularizadas: Ridge, Lasso y Elastic Net\n", "\n", "Este notebook compara el comportamiento, coeficientes, rendimiento y supuestos de los modelos de regresi\u00f3n regularizada Ridge, Lasso y Elastic Net, usando el mismo conjunto de datos estandarizado."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Cargar y preparar datos\n", "Usaremos el conjunto de datos de diabetes de `sklearn` y lo dividiremos en entrenamiento y prueba."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.datasets import load_diabetes\n", "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import mean_squared_error, r2_score\n", "\n", "X, y = load_diabetes(return_X_y=True)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "scaler = StandardScaler()\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Ajuste de modelos\n", "Utilizaremos valores de hiperpar\u00e1metros fijos comparables."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Modelos\n", "ridge = Ridge(alpha=1.0).fit(X_train_scaled, y_train)\n", "lasso = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n", "enet = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000).fit(X_train_scaled, y_train)\n", "\n", "# Predicciones\n", "y_pred_ridge = ridge.predict(X_test_scaled)\n", "y_pred_lasso = lasso.predict(X_test_scaled)\n", "y_pred_enet = enet.predict(X_test_scaled)\n", "\n", "# Errores\n", "def resumen(nombre, y_true, y_pred):\n", "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n", "    r2 = r2_score(y_true, y_pred)\n", "    print(f\"{nombre} \u2192 RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n", "\n", "resumen(\"Ridge\", y_test, y_pred_ridge)\n", "resumen(\"Lasso\", y_test, y_pred_lasso)\n", "resumen(\"Elastic Net\", y_test, y_pred_enet)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Comparaci\u00f3n de coeficientes\n", "Visualizamos las diferencias en magnitud y sparsity."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\n", "plt.plot(ridge.coef_, label='Ridge')\n", "plt.plot(lasso.coef_, label='Lasso')\n", "plt.plot(enet.coef_, label='Elastic Net')\n", "plt.axhline(0, color='gray', linestyle='--')\n", "plt.xlabel(\"\u00cdndice de variable\")\n", "plt.ylabel(\"Coeficiente\")\n", "plt.title(\"Coeficientes estimados\")\n", "plt.legend()\n", "plt.grid(True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. N\u00famero de coeficientes distintos de cero\n", "Esto mide cu\u00e1n esparso es el modelo (selecci\u00f3n de variables)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Coef. distintos de cero:\")\n", "print(\"Ridge:\", np.sum(ridge.coef_ != 0))\n", "print(\"Lasso:\", np.sum(lasso.coef_ != 0))\n", "print(\"Elastic Net:\", np.sum(enet.coef_ != 0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Conclusiones generales\n", "- **Ridge**: mantiene todos los coeficientes \u2192 \u00fatil si todas las variables aportan\n", "- **Lasso**: selecciona un subconjunto de variables \u2192 \u00fatil para interpretaci\u00f3n y reducci\u00f3n\n", "- **Elastic Net**: balance entre selecci\u00f3n y estabilidad \u2192 ideal con variables correlacionadas\n", "\n", "La elecci\u00f3n del modelo depende del contexto: precisi\u00f3n vs interpretabilidad vs colinealidad."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}{
   "cell_type": "markdown",
   "id": "c4540f8b",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusiones\n",
    "\n",
    "- **Ridge** mantiene todos los coeficientes peque√±os ‚Üí √∫til con multicolinealidad.\n",
    "- **Lasso** realiza selecci√≥n de variables autom√°tica.\n",
    "- **Elastic Net** balancea ambos efectos, √∫til con muchas variables correlacionadas.\n",
    "\n",
    "Este an√°lisis muestra c√≥mo cada t√©cnica responde a los mismos datos de forma diferente, y c√≥mo elegir entre ellas depende de los objetivos: estabilidad, interpretabilidad o predicci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d092e",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è Historia de los m√©todos\n",
    "\n",
    "| M√©todo        | A√±o  | Creador(es)             | Motivaci√≥n principal                          |\n",
    "|---------------|------|--------------------------|------------------------------------------------|\n",
    "| **Ridge**     | 1970 | Hoerl & Kennard          | Estabilizar estimaciones con colinealidad     |\n",
    "| **Lasso**     | 1996 | Robert Tibshirani        | Selecci√≥n autom√°tica de variables             |\n",
    "| **Elastic Net** | 2005 | Zou & Hastie            | Combinar estabilidad y esparsidad             |\n",
    "\n",
    "Estos m√©todos marcaron hitos importantes en la estad√≠stica moderna y aprendizaje autom√°tico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768d8f5",
   "metadata": {},
   "source": [
    "## üßæ Origen de los nombres\n",
    "\n",
    "- **Ridge**: ‚ÄúCresta‚Äù que estabiliza soluciones cuando hay colinealidad.\n",
    "- **Lasso**: Acr√≥nimo de *Least Absolute Shrinkage and Selection Operator*, tambi√©n sugiere un lazo que ata coeficientes a cero.\n",
    "- **Elastic Net**: Red el√°stica que combina las propiedades de Ridge y Lasso.\n",
    "\n",
    "Estos nombres reflejan tanto la geometr√≠a como la intenci√≥n estad√≠stica de cada m√©todo.\n"
   ]
  }
