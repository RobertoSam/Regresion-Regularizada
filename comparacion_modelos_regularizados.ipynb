{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaci√≥n de Regresiones Regularizadas: Ridge, Lasso y Elastic Net\n",
    "\n",
    "Este notebook compara el comportamiento, coeficientes, rendimiento y supuestos de los modelos de regresi√≥n regularizada Ridge, Lasso y Elastic Net, usando el mismo conjunto de datos estandarizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar y preparar datos\n",
    "Usaremos el conjunto de datos de diabetes de `sklearn` y lo dividiremos en entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ajuste de modelos\n",
    "Utilizaremos valores de hiperpar√°metros fijos comparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "ridge = Ridge(alpha=1.0).fit(X_train_scaled, y_train)\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "y_pred_enet = enet.predict(X_test_scaled)\n",
    "\n",
    "# Errores\n",
    "def resumen(nombre, y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{nombre} ‚Üí RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "resumen(\"Ridge\", y_test, y_pred_ridge)\n",
    "resumen(\"Lasso\", y_test, y_pred_lasso)\n",
    "resumen(\"Elastic Net\", y_test, y_pred_enet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparaci√≥n de coeficientes\n",
    "Visualizamos las diferencias en magnitud y sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ridge.coef_, label='Ridge')\n",
    "plt.plot(lasso.coef_, label='Lasso')\n",
    "plt.plot(enet.coef_, label='Elastic Net')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"√çndice de variable\")\n",
    "plt.ylabel(\"Coeficiente\")\n",
    "plt.title(\"Coeficientes estimados\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. N√∫mero de coeficientes distintos de cero\n",
    "Esto mide cu√°n esparso es el modelo (selecci√≥n de variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coef. distintos de cero:\")\n",
    "print(\"Ridge:\", np.sum(ridge.coef_ != 0))\n",
    "print(\"Lasso:\", np.sum(lasso.coef_ != 0))\n",
    "print(\"Elastic Net:\", np.sum(enet.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusiones\n",
    "- **Ridge** mantiene todos los coeficientes peque√±os ‚Üí √∫til con multicolinealidad.\n",
    "- **Lasso** realiza selecci√≥n de variables autom√°tica.\n",
    "- **Elastic Net** balancea ambos efectos, √∫til con muchas variables correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è Historia de los m√©todos\n",
    "| M√©todo        | A√±o  | Creador(es)             | Motivaci√≥n principal                          |\n",
    "|---------------|------|--------------------------|------------------------------------------------|\n",
    "| **Ridge**     | 1970 | Hoerl & Kennard          | Estabilizar estimaciones con colinealidad     |\n",
    "| **Lasso**     | 1996 | Robert Tibshirani        | Selecci√≥n autom√°tica de variables             |\n",
    "| **Elastic Net** | 2005 | Zou & Hastie            | Combinar estabilidad y esparsidad             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßæ Origen de los nombres\n",
    "- **Ridge**: ‚ÄúCresta‚Äù que estabiliza soluciones ante colinealidad.\n",
    "- **Lasso**: Acr√≥nimo de *Least Absolute Shrinkage and Selection Operator*.\n",
    "- **Elastic Net**: Red el√°stica que combina propiedades de Ridge y Lasso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
